# -*- coding: utf-8 -*-
"""new sentiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fn-W7cwlWRfojlJeoTeUrwylowFLdKEv
"""

from keras.models import Sequential
from keras.layers.core import Dense,Activation,Dropout
from keras.wrappers.scikit_learn import KerasClassifier
from keras.preprocessing import sequence
import keras
import tensorflow as tf
import numpy as np
from sklearn.svm import LinearSVC
import pandas as pd
import nltk
from nltk.corpus import twitter_samples
import matplotlib.pyplot as plt
import random
import re
from sklearn.neural_network import BernoulliRBM
import string
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import TweetTokenizer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression

from google.colab import drive

drive.mount('/content/gdrive')

root_path = 'gdrive/My Drive/'

all_data=pd.read_csv('/content/gdrive/MyDrive/4001k.csv',encoding='latin1')

all_data=all_data.dropna()

labels=all_data['Labels']
tweets=all_data['Tweets']
new_data=pd.DataFrame()
new_data[['Labels', 'Tweets']] = all_data[['Labels', 'Tweets']]
def clean1(txt):
    txt1=txt.lower()
    txt1=' '.join(re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)"," ",txt1).split())
    txt1=re.sub('[0-9]','',txt1)
    return txt1
new_data['Cleaned_1_Tweets']=new_data['Tweets'].apply(clean1)
# print(new_data.Labels.unique())
X=new_data['Cleaned_1_Tweets']
y=new_data['Labels']



new_data['Labels'].value_counts()

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder

# print(new_data.Labels.unique())
new_data=new_data.dropna()
X=new_data['Cleaned_1_Tweets']
X=X.dropna()
y=new_data['Labels']
y=y.dropna()

lst1=np.array(X)
print(len(lst1))
word2count={}
for tweet in lst1:
  for word in tweet.split():
      if word not in word2count:
          word2count[word]=1
      else:
          word2count[word]+=1
unique_words=len(word2count.keys())
tweet1int={}
word_number=0
threshold=5
for word,count in word2count.items():
    if count>=threshold:
        tweet1int[word]=word_number
        word_number+=1
        

tweet1int['<OUT>']=len(tweet1int) + 1
tweet2int=[]
for tweet in lst1:
    ints=[]
    for word in tweet.split():
        if word not in tweet1int:
            ints.append(tweet1int['<OUT>'])
        else:
            ints.append(tweet1int[word])
    tweet2int.append(ints)

new_data["Tweets_2_int"]=tweet2int

 
shuffled_X=new_data['Tweets_2_int']
shuffled_y=new_data['Labels']


npX=np.array(shuffled_X)
npY=np.array(shuffled_y)
X_train,X_test,y_train,y_test=train_test_split(npX,y,test_size=.20,random_state=42)
max_len=100
batch_size=128
X_train=sequence.pad_sequences(X_train,max_len)
X_test=sequence.pad_sequences(X_test,max_len)

unique_words=np.array(unique_words)
unique_words.shape

model=tf.keras.Sequential([
     tf.keras.layers.Embedding(unique_words+1,256),
     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128,return_sequences=True)),
     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32,return_sequences=True,dropout=0.2)),

     tf.keras.layers.Dense(32),
     tf.keras.layers.Dense(1,activation='relu')     
])
model.summary()

model.compile(loss="BinaryCrossentropy",optimizer="adam",metrics=['acc'])
history = model.fit(X_train, y_train, epochs=5,batch_size=128)
results = model.evaluate(X_test, y_test)
print(results)
model.save("new_Senti.h5")

def pred1(txt):
  word_index=tweet1int
  token=txt
  tokens=clean1(token)
  tokens=tokens.split()
  tokens=[word_index[word]if word in word_index else 0 for word in tokens]
  padded=sequence.pad_sequences([tokens],max_len)[0]
  pred=np.zeros((1,100))
  pred[0]=padded
  res=model.predict(pred)
    
  print(res[0][0][0])
  if res[0][0][0]<=[0.5]:
    print('Negative Statement')
  else:
    print('Positive Statement')

pred1('man this corona is spreading fear and it is not good')
